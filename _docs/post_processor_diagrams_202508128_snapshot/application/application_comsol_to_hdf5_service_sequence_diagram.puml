@startuml COMSOL to HDF5 Workflow
skinparam backgroundColor #FFFFFF

title Flux Détaillé de Conversion COMSOL → HDF5

' ═══════════════════════════════════════════════════════════
' ACTEURS
' ═══════════════════════════════════════════════════════════
actor Utilisateur as user
participant "ConversionService" as service
participant "CSVParser" as csvparser
participant "HTMLExtractor" as htmlextractor
participant "DataProcessor" as processor
participant "ComsolHDF5Exporter" as exporter
database "HDF5 Repository" as repo

' ═══════════════════════════════════════════════════════════
' SCÉNARIO 1 : CONVERSION SIMPLE (1 CSV + 1 HTML)
' ═══════════════════════════════════════════════════════════
group Conversion Simple
  user -> service : convert_comsol_to_hdf5(csv_path, html_path)
  activate service
  
  service -> service : validate_file_paths()
  service -> service : generate_simulation_id()
  
  ' ─────────────────────────────────────────────────────────
  ' ÉTAPE 1 : Parse CSV
  ' ─────────────────────────────────────────────────────────
  service -> csvparser : parse(csv_path)
  activate csvparser
  
  csvparser -> csvparser : read_file()
  csvparser -> csvparser : parse_header()\n[Model, Version, Date, Table]
  csvparser -> csvparser : parse_data_table()\n[positions, measurements]
  
  csvparser --> service : CSVData\n{header, metadata, dataframe}
  deactivate csvparser
  
  ' ─────────────────────────────────────────────────────────
  ' ÉTAPE 2 : Extract HTML
  ' ─────────────────────────────────────────────────────────
  service -> htmlextractor : extract(html_path)
  activate htmlextractor
  
  htmlextractor -> htmlextractor : parse_html()
  htmlextractor -> htmlextractor : extract_global_parameters()
  htmlextractor -> htmlextractor : extract_mesh_statistics()
  htmlextractor -> htmlextractor : extract_solver_config()
  htmlextractor -> htmlextractor : extract_geometry_info()
  
  htmlextractor --> service : HTMLData\n{params, mesh, solver, geometry}
  deactivate htmlextractor
  
  ' ─────────────────────────────────────────────────────────
  ' ÉTAPE 3 : Créer Domain Object
  ' ─────────────────────────────────────────────────────────
  service -> service : create_simulation_data(csv_data, html_data)
  note right
    ComsolSimulationData(
      simulation_id=<generated>,
      parameters=<from HTML>,
      raw_data=<from CSV>,
      metadata=<from HTML>
    )
  end note
  
  ' ─────────────────────────────────────────────────────────
  ' ÉTAPE 4 : Process Data (optionnel)
  ' ─────────────────────────────────────────────────────────
  service -> processor : process(simulation_data)
  activate processor
  
  processor -> processor : compute_potential_differences()
  processor -> processor : normalize_data()
  processor -> processor : validate_data()
  
  processor --> service : ProcessedResults\n{computed_fields}
  deactivate processor
  
  ' ─────────────────────────────────────────────────────────
  ' ÉTAPE 5 : Export to HDF5
  ' ─────────────────────────────────────────────────────────
  service -> exporter : export(simulation_data, processed_results, output_path)
  activate exporter
  
  exporter -> exporter : create_hdf5_file()
  exporter -> exporter : write_metadata_group()
  exporter -> exporter : write_raw_data_group()
  exporter -> exporter : write_processed_group()
  exporter -> exporter : set_file_attributes()
  
  exporter --> service : HDF5 File Path
  deactivate exporter
  
  ' ─────────────────────────────────────────────────────────
  ' ÉTAPE 6 : Persist to Repository
  ' ─────────────────────────────────────────────────────────
  service -> repo : register(simulation_id, hdf5_path, metadata)
  activate repo
  
  repo -> repo : update_index()
  repo -> repo : update_catalog()
  
  repo --> service : Success
  deactivate repo
  
  service --> user : Conversion successful\n{simulation_id, hdf5_path}
  deactivate service
end

' ═══════════════════════════════════════════════════════════
' SCÉNARIO 2 : CONVERSION BATCH (DIRECTORY)
' ═══════════════════════════════════════════════════════════
newpage Batch Conversion

actor Utilisateur as user2
participant "ConversionService" as service2
participant "BatchProcessor" as batch
database "HDF5 Repository" as repo2

group Conversion Batch
  user2 -> service2 : batch_convert_directory(directory_path)
  activate service2
  
  service2 -> batch : discover_file_pairs(directory_path)
  activate batch
  
  batch -> batch : scan_directory()
  batch -> batch : match_csv_html_pairs()
  batch -> batch : filter_already_converted()
  
  batch --> service2 : List[FilePair]
  deactivate batch
  
  loop For each file pair
    service2 -> service2 : convert_comsol_to_hdf5(csv, html)
    note right
      Réutilise le flux
      de conversion simple
    end note
  end
  
  service2 -> repo2 : build_catalog_index()
  activate repo2
  repo2 --> service2 : Catalog updated
  deactivate repo2
  
  service2 --> user2 : Batch completed\n{n_converted, n_failed, catalog_path}
  deactivate service2
end

' ═══════════════════════════════════════════════════════════
' SCÉNARIO 3 : QUERY & RETRIEVAL
' ═══════════════════════════════════════════════════════════
newpage Query & Retrieval

actor Utilisateur as user3
participant "QueryService" as query
database "HDF5 Repository" as repo3
database "HDF5 Files" as hdf5files

group Query Results
  user3 -> query : query_simulations(filters)
  activate query
  
  note right of query
    Exemple de filtres:
    {
      "object_type": "Parallelepiped",
      "x_objet": (-5, 5),
      "freq": 10000,
      "date_range": ("2025-12-01", "2025-12-31")
    }
  end note
  
  query -> repo3 : search_catalog(filters)
  activate repo3
  
  repo3 -> repo3 : load_catalog_index()
  repo3 -> repo3 : filter_by_criteria()
  
  repo3 --> query : List[SimulationMetadata]
  deactivate repo3
  
  loop For each matching simulation
    query -> hdf5files : load_simulation_data(simulation_id)
    activate hdf5files
    hdf5files --> query : SimulationData
    deactivate hdf5files
  end
  
  query -> query : aggregate_results()
  
  query --> user3 : QueryResults\n{simulations, aggregated_data}
  deactivate query
end

' ═══════════════════════════════════════════════════════════
' SCÉNARIO 4 : MERGE SIMULATIONS (RECOLLEMENT)
' ═══════════════════════════════════════════════════════════
newpage Merge & Recollection

actor Utilisateur as user4
participant "MergeService" as merge
database "HDF5 Repository" as repo4

group Merge Simulations
  user4 -> merge : merge_simulations(simulation_ids, merge_strategy)
  activate merge
  
  note right of merge
    Merge strategies:
    - SPATIAL_CONTINUITY (recollement spatial)
    - FREQUENCY_SWEEP (balayage fréquentiel)
    - PARAMETER_SWEEP (balayage paramétrique)
  end note
  
  loop For each simulation_id
    merge -> repo4 : load_simulation(simulation_id)
    activate repo4
    repo4 --> merge : SimulationData
    deactivate repo4
  end
  
  merge -> merge : validate_compatibility()
  merge -> merge : apply_merge_strategy()
  merge -> merge : handle_overlaps()
  merge -> merge : interpolate_gaps()
  
  merge -> repo4 : save_merged_simulation(merged_data)
  activate repo4
  repo4 --> merge : merged_simulation_id
  deactivate repo4
  
  merge --> user4 : MergedSimulation\n{merged_id, metadata}
  deactivate merge
end

@enduml

